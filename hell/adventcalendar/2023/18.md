---
title: "Test-driven HTML and accessibility"
layout: layouts/advent.md
author: "David Luhr"
author_bio: "David Luhr is an independent consultant who helps teams of all sizes with accessible design and development. He is passionate about creating a more responsible web for everyone, eliminating waste, and creating free educational content through his [Build UX YouTube channel](https://www.youtube.com/@buildux)."
date: 2023-12-18
tags: advent2023
author_links:
  - label: "Personal website and blog"
    url: "https://luhr.co"
    link_label: "luhr.co"
  - label: "YouTube"
    url: "https://www.youtube.com/@buildux"
    link_label: "youtube.com/@buildux"
  - label: "LinkedIn"
    url: "https://www.linkedin.com/in/davidluhr/"
    link_label: "linkedin.com/in/davidluhr"
active: true
intro: "<p>David demonstrates how to apply test-driven development techniques to test HTML across browsers with an accessibility-first workflow.</p>"
status:
  review_manuel: "done"
  review_eric: "done"
  review_saptak: "skipped"
---

<!-- Manuel: Super interesting post. Thanks a lot! -->

When I started writing unit tests and following a [test-driven development (TDD)](https://en.wikipedia.org/wiki/Test-driven_development) workflow, I was stoked with the immediate feedback and confidence I gained in every line of JavaScript I wrote.

<!-- Eric: This might be a reader's first encounter with TDD. Can we link to a high-level explanation of it to allow curious readers to self-serve? -->
<!-- David: Done! -->

TDD improved my software design with simpler, more predictable code. It saved me countless hours of manual debugging by reporting errors directly in failing tests. And with solid test coverage, it allowed me to add new logic and functionality while keeping everything in an always-working state.

I followed the process of [Red, Green, Refactor](https://www.jamesshore.com/v2/blog/2005/red-green-refactor), meaning I wrote a failing test, wrote just enough code to make the test pass, and then freely cleaned up my code, knowing everything was working. My JavaScript code had never been better.

<!--
  Eric:
  >TDD improved my software design, saved me countless hours of manual debugging
  How?
-->
<!-- David: I added some details above to elaborate on this. -->

<!-- Eric: Can we also link to a resource that gives more detail about what Red, Gree, Refactor is? -->
<!-- David: Done! -->

Then one day, I wrote a UI-oriented test that used `document.querySelector()`. Before the test even finished running, it errored and revealed that JavaScript tests run outside of the browser in Node, where no document object model (DOM) exists. Since the focus of my career is evaluating, designing, and developing accessible user interfaces, primarily with HTML and CSS, I wanted to apply TDD techniques to HTML and accessibility. But it seemed like my dream testing setup wasn't possible.

<!-- Eric: I might encourage a slight restructuring of this paragraph. I think the narrative being 1. Node doesn't allow for browser-run JS testing, 2. My career and its specialty could benefit from TDD, and then 3. At first it seemed like this wouldn't be possible. -->
<!-- David: I changed up the order as you suggested and it flows better. Thank you! -->

Months passed of continuing to enjoy TDD with JavaScript, while wishing I could do more to test my accessible markup. Most of my research pointed to slow, flaky approaches like scripting an entire [headless browser](https://en.wikipedia.org/wiki/Headless_browser) just to check if an HTML element rendered or if a menu opened on click.

Other testing libraries use synthetic (faked) interactions by dispatching DOM events that might lead to false positives in tests if they don't accurately replicate real user interaction. Plus, these libraries don't have the necessary DOM events for all interactions. I wanted to write small, focused, fast unit tests that provided trustworthy, immediate feedback with every change I made.

<!-- Eric: Can we link to what a headless browser is? -->
<!-- David: Done! -->

<!--
  Eric:
  >synthetic (faked) interactions that might instill false confidence
  How would they do this?
-->
<!-- David: I added some further explanation. If this comparison is a rabbit hole, I'm happy to not mention other tools and just focused on my recommend approach. Not sure how much "what about x?" I need to preempt. -->

Then I came across a tool called [Web test runner](https://modern-web.dev/docs/test-runner/overview/) from the team behind [Open Web Components](https://open-wc.org/). Instead of running JavaScript tests in Node, Web Test Runner executes tests directly in the browser.

This is a big deal for multiple reasons:

1. Tests run your production JavaScript code in the production environment&mdash;the browser
2. Tests have access to the real DOM, unlocking the ability to write tests for HTML, accessibility, and even CSS
3. Tests can send native interactions, such as actual mouse clicks and keyboard input, instead of faking them with DOM events
4. And it's even possible do things like set the viewport dimensions to test responsive design

<!-- Manuel: "of native (not faked) interaction events" <- the difference needs explaining. -->
<!-- David: I added some clarification around this, but it's a nuanced topic. Playwright has direct access to the browser's input pipeline (perform a mouse click at this coordinate in the window), whereas other testing libraries send DOM events that you'd _expect_ to fire _after_ an interaction like a mouse click occurs. There's a risk of these manually composed DOM events not matching the actual browser's behavior. Per my response to Eric above, I can cut this comparison if it's too much to discuss. -->

<!-- Eric: I just want to take this space for communicating my appreciation of the use of an emdash ðŸ˜ƒ -->
<!-- David: I'm glad you're a fellow emdash fan! -->

## Shift accessibility feedback earlier

With this approach, I write tests to capture my expectations around semantic markup and accessibility upfront, then get instant feedback on whether my code meets those expectations.

Let's say I expect paragraph (`<p>`) elements shouldn't be empty. I can write the following test, which gets all `<p>` elements, loops over them, and expects each one doesn't have empty text content:
```js
describe("paragraphs", () => {
	const docAllParagraphs = Array.from(document.querySelectorAll("img"));

	it("have text content", () => {
		docAllParagraphs.forEach((paragraph) => {
			expect(paragraph.textContent).to.not.equal("");
		});
	});
});
```

<!-- Eric: A short paragraph here that describes what those expectations are and how they translate to code might be helpful here. -->
<!-- David: Good call. I added a simple example to prime readers on how this works. -->

With access to the real DOM in tests, I can continuously check if I'm using heading levels, ARIA, and landmark regions correctly. I can build custom UI patterns and ensure they respond to keyboard input and manage focus as expected. And as I add new features or refactor my code, my tests re-run with every change to make sure I don't harm accessibility as the project evolves.

<!-- Eric: I'm curious now about what harming accessibility might look like from a mature TDD-mindset. -->
<!-- David: I'm not sure I understand "harming accessibility". Do you mean is it possible for developers who follow TDD to creating accessibility issues in their code? -->

Accessibility-focused linters give feedback as you code, but have limited awareness outside of very specific violations. Automated accessibility audits and most manual review happens after you write the code. Test-driven accessibility is about providing continuous feedback before, during, and after you write the code.

Using all these forms of accessibility testing together provides the most value. Unit testing HTML allows me find, fix, and prevent accessibility issues as I'm working. It also allows me to focus my manual accessibility reviews on more holistic evaluations, such as navigating through a page with a screen reader to asses the user experience.

This test-driven approach makes something possible that I've been striving for in my work: an accessibility-first workflow. With an accessibility-first mindset, we plan for and preserve accessibility from the beginning of a project instead of leaving it as an afterthought. If I could achieve one thing in my career, it would be to make accessibility-first a foundational practice alongside usability, responsive design, performance, and security.

<!-- Eric: Love this. -->

## Evolve code with confidence

In addition to getting feedback upfront and continuously thereafter, having tests allows us to change our code freely as features and user needs evolve.

The downside of automated accessibility testing tools and manual review is they can't be performed continuously, and often focus on the current page in the browser. We either have to manually re-test with each change we make exhaustively across all pages, or we depend on getting feedback after batches of changes.

With tests in place, we can keep our code (and accessibility) in an always-working state. Every time we make a change, our tests run across the entire codebase. If our tests pass, we can more confidently and frequently deploy our changes to users.

## The setup

To begin testing our HTML in the browser, we first install our dependencies:

```bash
npm i --save-dev @web/test-runner @web/test-runner-mocha @esm-bundle/chai @web/test-runner-commands @web/test-runner-playwright
```

We need [Web test runner](https://modern-web.dev/docs/test-runner/overview/) to run tests in the browser, Web Test Runner's implementation of Mocha for directly testing HTML files, Chai for test assertions, Web Test Runner commands for sending native interactions (click, keyboard, etc.) to our UI elements, and Web Test Runner's implementation of Playwright for headless browsers to run the tests in.

<!-- Eric: Can we link to https://modern-web.dev/docs/test-runner/overview/ ? Assuming this is that. -->
<!-- David: Good call. Done! -->

With this setup, we run our tests in Chromium, Firefox, and WebKit all at once to make sure our code works across the major browsers. This happens super quickly in the background and works in different operating systems.

Even through browser cross-compatibility continues to improve, this testing setup reveals discrepancies that would be incredibly difficult to otherwise detect. For example, I recently found that getting the value of an element's `role` attribute with `Element.role` was failing only in Firefox, and using `Element.getAttribute("role")` was the fix. I'm not sure if or how I would have found that issue without tests that run directly in each of the browsers.

<!--
  Eric:
  >this testing setup reveals discrepancies that would be incredibly difficult to otherwise detect
  I'd love an example of this, as this is a very niche, obscure corner of the industry that not a lot of folks are familiar with.
-->
<!-- David: I added a recent example. Hope that helps. -->

Next, we add a couple scripts to our `package.json` to run tests either in a single pass or in watch mode:

```json
"scripts": {
  "test": "web-test-runner \"/**/*.test.js\" --node-resolve --playwright --browsers chromium firefox webkit",
  "test:watch": "web-test-runner \"/**/*.test.js\" --node-resolve --playwright --browsers chromium firefox webkit --watch"
},
```

Last, we can run our watch mode script as we work. This will run all tests, then detect any file changes and re-run only the impacted tests:

```bash
npm run test:watch
```

If our website is made with simple HTML pages, we can run the tests directly in the HTML files (with a `<script>` tag) to test specific markup. If our HTML is rendered with JavaScript, as is the case with web components and many other approaches, then we can test the rendering in JS files alongside any logic or functionality that we'd usually unit test. We'll compare the tradeoffs of these two options at the end of this article.

## Writing tests

To demonstrate the value of this setup, let's write tests to prevent some of the most common (non-visual) WCAG 2 failures found in the annual [WebAIM Million study](https://webaim.org/projects/million/#wcag):

<!-- Manuel. Great idea! -->

- Low contrast text
- Missing alt text
- Empty links
- Missing form input labels
- Empty buttons
- Missing document language

<!-- Manuel: Why did you exclude low contrast text? -->
<!-- David: There's a way to test for low contrast text, but it would be too complex for the scope of this post. I added a qualifying statement below to explain this. -->

Writing tests for low contrast text is possible but too involved for this post. So let's focus on tests for the remaining most common failures.

First, we'll create a test file called `example.test.js` with the required imports:

```js
import { runTests } from "@web/test-runner-mocha";
import { expect } from "@esm-bundle/chai";
import { sendKeys } from "@web/test-runner-commands";

runTests(() => {
  // TODO: Write tests here
});
```

`runTests` allows us to directly test HTML files, which makes this setup quick and easy for static HTML. `@web/test-runner-commands` provides native interaction utilities, such as `sendKeys`, that we'll use in a later example.

Then we'll create an example HTML file called `example.html` with the usual boilerplate and link to the test file:

```html
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Our test file</title>

    <script type="module" src="example.test.js"></script>
  </head>
  <body>
    <!-- TODO: Write markup here -->
  </body>
</html>
```

### Missing alt text

In `example.test.js`, let's write a test to check for missing image alt text:

```js
// ...
runTests(() => {
  describe("images", () => {
    const docAllImages = Array.from(document.querySelectorAll("img"));

    it("have an alt attribute", () => {
      docAllImages.forEach((image) => {
        expect(image.getAttribute("alt"), image.outerHTML).to.exist;
      });
    });
  });
});
```

This test gets all images in the DOM, loops over them, and checks to make sure each one has an `alt` attribute.

Error messages in JavaScript include the line and character numbers to help quickly locate and fix errors, but we don't have access to this with HTML elements. By passing `image.outerHTML` as the second argument of our `expect()` function, we can include the element in the error message to make finding and correcting the offending element easier. We'll use this for custom error messages any time we're checking all instances of an element.

If this is too verbose or noisy for more complex elements, we could instead only include the element's opening tag, or assign all elements unique `data-test-id` values to identify them directly in test error messages.

<!-- Eric: This is a technical-oriented paragraph. Because of this, I might advise splitting it into two shorter paragraphs, each with only one main thought. -->
<!-- David: Good idea. I broke the ideas into separate paragraphs. -->

In our `example.html` file, let's add an image without an `alt` attribute to get a failing test:

```html
<img src="/example.jpg" />
```

Web Test Runner reports the following in the command line:

```
âŒ images > have an alt attribute
	AssertionError: <img src="/example.jpg">: expected null to exist
		at example.test.js:9:41

Chromium: 1 failed
Firefox: 1 failed
Webkit: 1 failed
```

Our test fails as expected. To get the test to pass, we simply need to add an `alt` attribute, either as an empty string `""` for decorative content or with a useful description for meaningful content.

The neat thing about this test is it passes if no `<img>` elements exist, but fails if they exist and don't have an `alt` attribute. So, this test can be used from the beginning of any project without creating irrelevant noise.

Let's write tests for other common WCAG 2 failures.

### Empty links

We'll add another test in `example.test.js`:

```js
runTests(() => {
  // ...
  describe("links", () => {
    const docAllLinks = Array.from(document.querySelectorAll("a"));

    it("have a non-empty href attribute", () => {
      docAllLinks.forEach((link) => {
        const hrefValue = link.getAttribute("href");
        expect(hrefValue, link.outerHTML).to.exist;
        expect(hrefValue, link.outerHTML).to.not.equal("");
      });
    });

    it("are not empty", () => {
      docAllLinks.forEach((link) => {
        expect(link.textContent, link.outerHTML).to.not.equal("");
      });
    });
  });
});
```

Similar to the previous test, we get all links from the DOM, loop over them, and check that their text content is not empty. We're also checking that links have a non-empty `href` attribute, just as an example of other HTML validation we'd like to do.

### Missing form input labels

Time for another test suite:

```js
runTests(() => {
  // ...
  describe("form inputs", () => {
    const docAllFormInputs = Array.from(
      document.querySelectorAll("input, textarea, select")
    );

    it("have a dedicated label", () => {
      docAllFormInputs.forEach((formInput) => {
        const inputId = formInput.id;
        expect(inputId, formInput.outerHTML).to.exist;

        const inputLabel = document.querySelector(`label[for="${inputId}"]`);
        expect(inputLabel, formInput.outerHTML).to.exist;
        expect(inputLabel.textContent, formInput.outerHTML).to.not.equal("");
      });
    });
  });
});
```

In this test, we get all form input elements, loop over them, and assert multiple things:

1. Each form input element needs an `id` attribute
2. We expect to find a `<label>` element with a `for` attribute that has the corresponding input's `id` as the value
3. We expect the `<label>` element to not be empty

It's possible to provide an accessible name for form inputs with other techniques, such as `aria-label` or `aria-labelledby`, but I'm being opinionated here because I prefer actual `<label>` elements and text content. Testing for an accessible name through other means is just as easy.

<!-- Manuel: "Missing form input labels" isn't as much about actual labels, but accessible names. This test would create false positives for people who use aria-label or aria-labelledby for naming in some parts of their UI. Could be worth mentioning. -->
<!-- David: Good point. I added some opinion above to address this. -->

<!-- Eric: At this point in the article I'm wondering what the difference is between this and running axe tools as you dev, or waiting for a CI check. Because of this, I'm wondering if you should spend a little more time in the intro portion of this post explaining the benefits and specifically naming why this is worth doing. -->
<!-- David: I incorporated this and other feedback you provided to cover these benefits earlier and emphasize that this approach is continuous, immediate, and checks all code/components/pages at once. -->

### Empty buttons

This is nearly identical to our empty links test:

```js
runTests(() => {
  // ...
  describe("buttons", () => {
    const docAllButtons = Array.from(document.querySelectorAll("button"));

    it("are not empty", () => {
      docAllButtons.forEach((button) => {
        expect(button.textContent, button.outerHTML).to.not.equal("");
      });
    });
  });
});
```

<!-- Manuel: A button has an implicit submit type in the context of a form. Nothing wrong about not setting it explicitly, or am I wrong? -->
<!-- David: You're correct. I removed that assertion. It's my personal preference to be explicit, but I shouldn't have included it in this post. -->

### Missing document language

Let's write a test for the final common WCAG 2 failure:

```js
runTests(() => {
  // ...
  describe("document", () => {
    it("has a set language", () => {
      const languageValue = document.querySelector("html").getAttribute("lang");

      expect(languageValue).to.equal("en");
    });
  });
});
```

This one is the most straightforward, but valuable nonetheless. If your site has internationalization, you'd want to check the `lang` value matches the current locale and updates based on user preference.

<!-- Manuel: but this test checks for English explicitly. -->
<!-- David: Yes. That's why I added the paragraph above that mentions what's required for internationalization. If a site is only authored in English, then this test respects that. If the language is changeable, then further consideration is needed. -->

## Testing heading levels

We're starting to build a universal test suite that will preserve accessibility across our projects.

Let's add some further assertions around heading levels, which are a common source of errors in accessibility audits:

```js
runTests(() => {
  // ...
  describe("headings", () => {
    const docAllHeadings = Array.from(
      document.querySelectorAll("h1, h2, h3, h4, h5, h6")
    );

    it("have <h1> element as the first heading", () => {
      expect(
        getHeadingLevel(docAllHeadings[0]),
        docAllHeadings[0].outerHTML
      ).to.equal(1);
    });

    it("have a single <h1>", () => {
      docAllHeadings.forEach((heading, index) => {
        // Don't fail the test if the first heading on the page is `<h1>`
        if (index === 0 && getHeadingLevel(heading) === 1) {
          return;
        }

        expect(getHeadingLevel(heading), heading.outerHTML).to.not.equal(1);
      });
    });

    it("don't skip heading levels", () => {
      docAllHeadings.forEach((heading, index) => {
        let previousHeadingLevel = 0;
        const currentHeadingLevel = getHeadingLevel(heading);

        if (index !== 0) {
          previousHeadingLevel = getHeadingLevel(docAllHeadings[index - 1]);
        }

        expect(currentHeadingLevel, heading.outerHTML).to.be.lessThanOrEqual(
          previousHeadingLevel + 1
        );
      });
    });
  });
});

function getHeadingLevel(heading) {
  return +heading.tagName.toLowerCase().replace("h", "");
}
```

With these tests, we can ensure heading levels are used properly in our page. We should only have a single `<h1>` heading. This heading should also be the first heading on the page. Lastly, headings should never skip levels, such as `<h2>` followed by `<h4>`. We make use of small utility function, `getHeadingLevel()`, to keep our code more concise and mistake-proof.

Although these tests enforce correct heading logic, they can't evaluate if heading levels are appropriate for the content they describe. That always requires thoughtful consideration. Tests free the developer to spend more time on wider UX considerations like this.

<!-- Eric: I might suggest heading some criticism off at the past and adding another benefit: this frees the developer to focus on things that can't be caught by testing/automation: that the heading hierarchy makes sense for the content. This touches on the UX of the experience that you mentioned earlier. -->
<!-- David: I like that and agree. The hope with any automation is that we're free to focus on more novel, meaningful work. -->

<!-- David: I removed the example of testing for list semantics since it's more unsettled as a practice and it trims the post length down. A better approach might be to _not_ expect the use `list-style: none;` or `list-style-type: none;` on `ul`/`ol`/`li` elements. -->

## Testing user interaction

So far, we've made simple assertions about our static HTML to make sure it's valid and not causing common accessibility failures. But as we create interactive patterns in our UI, the accessibility considerations become much more complex. We need to use HTML and ARIA to create custom semantics. We need to handle click, tap, and keyboard events. And we need to display and hide content, all while updating multiple attributes in concert.

<!-- Eric: Every semicolon is an opportunity to revise to have a clearer thought. Can we update this to use shorter, more direct sentences? -->
<!-- David: I agree. Semicolons are a "prose smell". I simplified things. -->

Let's follow the Red, Green, Refactor workflow as we build a custom disclosure (show/hide) pattern.

We'll start with our tests in `example.test.js` to make sure our initial HTML and ARIA are correct:

```js
runTests(() => {
  // ...
  describe("disclosure", () => {
    const docDisclosureToggle = document.querySelector(
      `[data-component="disclosureToggle"]`
    );
    const docDisclosureContent = document.querySelector(
      `[data-component="disclosureContent"]`
    );

    it("has a toggle button", () => {
      expect(docDisclosureToggle).to.exist;
      expect(docDisclosureToggle.tagName).to.equal("BUTTON");
    });

    it("has a toggle button with the expected ARIA attributes", () => {
      expect(docDisclosureToggle.getAttribute("aria-expanded")).to.exist;
      expect(docDisclosureToggle.getAttribute("aria-expanded")).to.equal(
        "false"
      );
      expect(docDisclosureToggle.getAttribute("aria-controls")).to.exist;
      expect(docDisclosureToggle.getAttribute("aria-expanded")).to.not.equal(
        ""
      );
    });

    it("has a content panel with the corresponding controls ID", () => {
      expect(docDisclosureContent).to.exist;
      expect(docDisclosureContent.id).to.equal(
        docDisclosureToggle.getAttribute("aria-controls")
      );
    });

    it("has a hidden content panel by default", () => {
      expect(docDisclosureContent.getAttribute("hidden")).to.exist;
    });
  });
});
```

<!-- Manuel: Shouldn't it also test whether the button is an actual <button>? -->
<!-- David: Good call. I've added that assertion. -->

These tests will fail, meaning we're in the "Red" stage. Now, we can author our initial HTML in `example.html` to get our tests to pass:

```html
<button
  type="button"
  aria-expanded="false"
  aria-controls="disclosureContent"
  data-component="disclosureToggle"
>
  Open disclosure
</button>

<div id="disclosureContent" data-component="disclosureContent" hidden>
  <p>Disclosure content</p>
</div>
```

Our tests now pass, so we're in the "Green" stage. I'd make a Git commit at this point. If there are any improvements we'd like to make, we can make them with confidence as long as our tests are passing, and commit again each time we're in a working state.

<!-- Eric: Nit, but I don't think I've ever heard/read the term "refactorings" before. What do you think about replacing it with the phrase "any opportunities for refactoring or other improvements"? -->
<!-- David: I've simplified the language just to "improvements". -->

<!--
  Eric:
  >we can make them with confidence as long as our tests are passing, and commit again each time we're in a working state
  This is great, and I feel like we've kind of buried the lede with having this so far down in the depths of the post. Referencing my earlier comments, what do you think about explaining this earlier on in terms of value to the developer? -->
<!-- David: I now introduce this benefit earlier with other benefits, per your suggestion. -->

Let's write a failing test to begin interacting with our disclosure in `example.test.js`:

```js
runTests(() => {
  // ...
  describe("disclosure", () => {
    // ...
    it("opens the disclosure on keyboard Enter press", async () => {
      docDisclosureToggle.focus();

      await sendKeys({
        down: "Enter",
      });

      expect(docDisclosureToggle.getAttribute("aria-expanded")).to.equal(
        "true"
      );
      expect(docDisclosureContent.getAttribute("hidden")).to.not.exist;

      // TODO: Reset the disclosure
    });
  });
});
```

With this test, we move focus to our toggle button, then use the `sendKeys` function from Web Test Runner commands to send a native keyboard `Enter` key press. We're checking that our toggle button gets updated attributes, and that our disclosure content is no longer hidden. We'd want similar tests for mouse click and the keyboard `Space` press, as well as testing that the disclosure closes on these interactions if it's already open.

We're in the "Red" stage again, but I'll leave the rest of the work to you to add the remaining tests and get them to pass. A cool thing is that the functionality needed for this component to work also creates some handy utility functions for our tests, such as resetting the disclosure at the end of each test to create a consistent starting point for other tests. And these utilities are easy to unit test with this approach as well.

<!-- Eric: Why is resetting state valuable? -->
<!-- David: I added a brief explanation above. -->

## Wrapping up

This workflow for test-driven HTML and accessibility provides so much value in my daily work, and I hope teams can adopt this technique to create a more responsible and usable web for everyone. I've used this setup extensively in large projects with complex UI patterns and it has scaled gracefully with several hundred tests running at all times.

With this setup, we can write unit tests that have access to the real DOM across major browsers, which allows us to check our HTML for validity, interactivity, and accessibility. We can run these tests in multiple browsers at once with every change, and preserve accessibility from the very start of our project. These expectations are permanently captured in our code, so we can freely refactor our work and add new features without introducing regressions.

<!-- Eric: Why is it important to have access to the real DOM? -->
<!-- David: Made a small edit to clarify that having access to the real DOM is what enables us to write these tests in the first place. -->

The examples in this post assume a static HTML file that we directly linked our tests to. This is the fastest and most direct way to test our HTML, but we don't want our test file to load in production. As a result, we'd need to remove the `<script>` tag before deploying to the web. To make this approach more convenient, it'd be good to do this automatically with a build command that creates a `/dist` folder or something similar.

Most projects probably won't just have static HTML files, and instead render components with JavaScript. In this case, we create standalone `.test.js` files that import corresponding JavaScript modules and call render functions or methods directly. From there, the workflow and benefits are all the same, but there may be a little more effort in setting up specific UI situations to test vs. having HTML ready to go.

As a final consideration, this testing technique _enhances_, not _replaces_, other forms of accessibility testing and shifts a lot of the feedback earlier in the process. Using accessibility-focused code linters, automated accessibility testing tools, and manual accessibility review together provides the most value.

<!-- Manuel: That's important because writing your own tests isn't easy. axe runs most of the tests in this post and probably does a better job simply because they have years of experience testing, tweaking, and adapting, and they also have more expertise than the average developer. -->
<!-- David: I agree axe is a very useful tool and does a great job with the common WCAG failures I cover in this post. I covered those failures because they're easier to understand as examples. The real value of this workflow is preventing failures with more involved situations, such interactive UI. -->

<!-- Eric: Ah, my earlier comments are now addressed! This is nice to know, but I think it's happening too late. This post is effectively asking people to expend effort to change their workflows and also write more code. Because of that, I think it's important to sell the person on _why_ before launching into _how_. -->
<!-- David: I moved this point earlier in the post and bolstered it with other benefits. -->

Many of the examples we covered in this post are checks we would otherwise have to make manually, either through code review or running accessibility audit tools. By writing unit tests for accessibility, we can discover and fix more issues as they arise. And we free more capacity to manually evaluate other aspects of accessibility, such as using assistive technology to evaluate the flow of a page or automated testing tools to check color contrast.

Automated tools cover about [30% of the WCAG success criteria and detect about 57% of overall issues](https://www.deque.com/automated-accessibility-testing-coverage/), leaving the rest for manual evaluation. With this approach, unit tests can increase coverage, particularly with interactive UI patterns. Even with more automated test coverage, manual review remains the most effective accessibility testing technique for finding issues and building empathy.

<!-- Manuel:
"20% to 30%" <- Where did you get these numbers?
"50/50" <- How did you get this number?
-->
<!-- David: I added a link and some clarification from a Deque report. -->

My long-term goal with this work is to build a robust, universal accessibility test collection that can be used across projects. As we explored with our disclosure example, it's also possible to create test suites for custom UI patterns such as accordions, menus, tooltips, and others.

<!-- Eric: I don't think you've socialized the HTML validator enough earlier on to communicate its importance, so its inclusion here is a bit of a surprise. Is there a way earlier you can speak to that? Maybe in terms of value because TDD allows you to focus more granularly or on components rather than whole pages? -->
<!-- David: I removed that mention to keep this post focused. Although valid HTML and accessibility overlap, it's an adjacent goal of creating this test suite and not essential to this post. -->

Let me know if you'd be interested in using this test collection or if you'd like to collaborate. Together, we can make test-driven accessibility and accessibility-first development a reality.

<!-- Manuel: Yes, that would be great and, honestly, critical! Although I find your post interesting and I find it great that you can write tests like these, I'd never suggest my clients to do that. It would be way too much effort to write tests that at best would be good enough. It would take years to write really good tests that consider all different possibilities (we saw some examples in your tests) and we probably need (better) support for AOM. It's hard to sell when you can run most tests with a free tool and catch the rest using manual testing, but if it was a group effort and open source, it would be different. -->
<!-- David: Fair points. The examples in this post are intentionally simplified to teach how the approach works and inspire people to explore what's possible, but of course there are many more considerations needed for robust testing. I am motivated to create a test suite that we can collectively invest in and benefit from. -->
